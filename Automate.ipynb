{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install pytorch-pretrained-bert==0.6.2\n",
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "!pip install flashtext\n",
    "!python -m spacy download en\n",
    "!python -m nltk.downloader universal_tagset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preparing BERT to return top N choices for a blanked word in a sentence. \n",
    "Eg: **Input**: The Sun is more ____ 4 billion years old. \n",
    "    **Output**: [than, like, of .....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-42e5abd5002b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBertForMaskedLM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Load pre-trained model tokenizer (vocabulary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer,BertForMaskedLM\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "import time\n",
    "start = time.time()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "end = time.time()\n",
    "print (\"Time Elapsed to load BERT \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to predict the top 30 choices for the fill in the blank word using BERT. \n",
    "# Eg: The Sun is more ____ 4 billion years old.\n",
    "\n",
    "def get_predicted_words(text):\n",
    "    text = \"[CLS] \" + text.replace(\"____\", \"[MASK]\") + \" [SEP]\"\n",
    "    # text= '[CLS] Tom has fully [MASK] from his illness. [SEP]'\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    #print(\"tokenized sentence: \",tokenized_text,\"\\n\")\n",
    "    masked_index = tokenized_text.index('[MASK]')\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    # Create the segments tensors.\n",
    "    segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "    # Get 30 choices for the masked(blank) word \n",
    "    k = 30\n",
    "    predicted_index, predicted_index_values = torch.topk(predictions[0, masked_index], k)\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_index_values.tolist())\n",
    "    filtered_tokens_to_remove_punctuation = []\n",
    "    # Remove any predictions that contain punctuation etc as they are not relevant to us.\n",
    "    for token in predicted_tokens:\n",
    "        if re.match(\"^[a-zA-Z0-9_]*$\", token):\n",
    "            filtered_tokens_to_remove_punctuation.append(token)\n",
    "        \n",
    "    return filtered_tokens_to_remove_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Computer is an ____ devise\"\n",
    "print (\"original sentence: \",sentence,\"\\n\")\n",
    "predicted_words = get_predicted_words(sentence)\n",
    "print (\"predicted choices: \", predicted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extracting important keywords (adpositions) and corresponding sentences from a story/article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an article from a file\n",
    "file_path = \"sun.txt\" #other texts in same directory: \"PSLE.txt\", \"hellenkeller.txt\", \"Grade7_electricity.txt\" , \"material.txt\", \"paperboat.txt\"\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "        return content\n",
    "    \n",
    "text = read_file(file_path)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  We will extract some adpositions. An adposition is a cover term for prepositions and postpositions.\n",
    "import pke\n",
    "import string\n",
    "\n",
    "def get_adpositions_multipartite(text):\n",
    "    out=[]\n",
    "\n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "    extractor.load_document(input=text)\n",
    "    #    not contain punctuation marks or stopwords as candidates.\n",
    "    pos = {'ADP'} #Adpositions\n",
    "    stoplist = list(string.punctuation)\n",
    "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "    extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "    # 4. build the Multipartite graph and rank candidates using random walk,\n",
    "    #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
    "    #    threshold/method parameters.\n",
    "    extractor.candidate_weighting(alpha=1.1,\n",
    "                                  threshold=0.75,\n",
    "                                  method='average')\n",
    "    keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "    for key in keyphrases:\n",
    "        out.append(key[0])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "adpositions = get_adpositions_multipartite(text)\n",
    "print (\"Adpositions from the text: \",adpositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the sentences for a given adpostion word. So each word may have mulitple sentences.\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    sentences = [sent_tokenize(text)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    # Remove any short sentences less than 20 letters.\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
    "    return sentences\n",
    "sentences = tokenize_sentences(text)\n",
    "\n",
    "def get_sentences_for_keyword(keywords, sentences):\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    keyword_sentences = {}\n",
    "    for word in keywords:\n",
    "        keyword_sentences[word] = []\n",
    "        keyword_processor.add_keyword(word)\n",
    "    for sentence in sentences:\n",
    "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
    "        for key in keywords_found:\n",
    "            keyword_sentences[key].append(sentence)\n",
    "\n",
    "    for key in keyword_sentences.keys():\n",
    "        values = keyword_sentences[key]\n",
    "        values = sorted(values, key=len, reverse=True)\n",
    "        keyword_sentences[key] = values\n",
    "    return keyword_sentences\n",
    "\n",
    "keyword_sentence_mapping_adpos = get_sentences_for_keyword(adpositions, sentences)\n",
    "\n",
    "for word in keyword_sentence_mapping_adpos:\n",
    "    print (word, \" : \",keyword_sentence_mapping_adpos[word],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For every adposition word we have multiple sentences. For every sentence we blank the adposition word and ask BERT \n",
    "#  to predict the top N choices. Then make a note of index of the correct answer in the predicitons. Then we sort the\n",
    "# sentences by the index and pick the top one.\n",
    "def get_best_sentence_and_options(word, sentences_array):\n",
    "    keyword = word\n",
    "    sentences = sentences_array\n",
    "    sentences = sorted(sentences, key=len, reverse=False)\n",
    "    max_no = min(5, len(sentences))\n",
    "    sentences = sentences[:max_no]\n",
    "    choices_filtered = []\n",
    "    ordered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        insensitive_line = re.compile(re.escape(keyword), re.IGNORECASE)\n",
    "        no_of_replacements =  len(re.findall(re.escape(keyword),sentence,re.IGNORECASE))\n",
    "        #blanked_sentence = sentence.replace(keyword, \"____\", 1)\n",
    "        blanked_sentence = insensitive_line.sub(\"____\", sentence)\n",
    "        blanks = get_predicted_words(blanked_sentence)\n",
    "\n",
    "        if blanks is not None:\n",
    "            choices_filtered = blanks\n",
    "            try:\n",
    "                word_index = choices_filtered.index(keyword.lower())\n",
    "                if no_of_replacements<2:\n",
    "                    ordered_sentences.append((blanked_sentence, choices_filtered, word_index))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    ordered_sentences = sorted(ordered_sentences, key=lambda x: x[2])\n",
    "    if len(ordered_sentences) > 0:\n",
    "        return (ordered_sentences[0][0], ordered_sentences[0][1])\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "for each_adpos in adpositions:\n",
    "    sentence, best_options = get_best_sentence_and_options(each_adpos, keyword_sentence_mapping_adpos[each_adpos])\n",
    "    print (sentence)\n",
    "    print (best_options)\n",
    "    print (\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
